# ğŸ—£ï¸ Multi-Speaker Text-to-Speech System

## ğŸ¯ Project Summary
This project implements a **natural voice synthesis system** capable of **voice cloning** using **pre-computed speaker embeddings**. Built using **Python**, **PyTorch**, **Coqui TTS**, **SpeechBrain**, and **Streamlit**, it allows users to input text and synthesize speech in the voice of different speakers.

---

## ğŸ§  Key Features

- ğŸ”Š Text-to-speech synthesis with **Coqui TTS (Glow-TTS)**
- ğŸ§¬ Speaker embedding extraction using **SpeechBrain**
- ğŸ—‚ï¸ 50-speaker dataset with metadata and preprocessing
- ğŸ’» Streamlit GUI for real-time interaction and audio playback
- ğŸ¯ Supports basic **voice cloning** via speaker embeddings

---

## ğŸ§ª Objectives

- Preprocess audio data: resampling and metadata creation
- Extract speaker embeddings from 50-speaker dataset
- Integrate embeddings into the TTS inference engine
- Build a functional GUI using Streamlit
- Evaluate generated speech for intelligibility and speaker similarity

---

## ğŸ› ï¸ Project Structure
multi-speaker-tts-system/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ speaker_dataset/ # Raw audio files + metadata.csv
â”‚
â”œâ”€â”€ preprocessing/
â”‚ â”œâ”€â”€ create_metadata.py # Metadata generation
â”‚ â”œâ”€â”€ preprocess_audio.py # Audio resampling (22050Hz)
â”‚ â””â”€â”€ compute_embeddings.py # Extracts & stores speaker embeddings
â”‚
â”œâ”€â”€ inference/
â”‚ â”œâ”€â”€ tts_engine.py # Core TTS engine using Coqui
â”‚ â””â”€â”€ infer.py # Wrapper for inference functions
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ app.py # Streamlit frontend
â”‚
â”œâ”€â”€ requirements.txt # Project dependencies
â”œâ”€â”€ runtime.txt # Python version pinning for deployment
â””â”€â”€ main.py # Entry point

---

## ğŸ–¼ï¸ User Interface (Streamlit)

- ğŸ”¤ Input text
- ğŸ‘¤ Select speaker (from dropdown)
- â–¶ï¸ Generate and play audio
- ğŸ’¾ Download synthesized audio

---

## ğŸ§ª Results

- Functional pipeline validated with listening tests.
- Speech is intelligible and voice cloning reflects speaker characteristics.
- Perceived speaker similarity varies across the dataset.

---

## ğŸš§ Limitations

- Some speaker clones lack strong perceptual similarity.
- Coqui Glow-TTS model produces moderately natural speech by default.
- Python 3.12 is **not supported** by `TTS`. Use **Python 3.10 or 3.9**.

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

git clone https://github.com/your-username/multi-speaker-tts-system.git
cd multi-speaker-tts-system

### 2. Create a Virtual Environment with Python 3.10

conda create -n tts-env python=3.10
conda activate tts-env

### 3. Install Dependencies

pip install -r requirements.txt

### 4. Run the App

streamlit run main.py


## ğŸ“š Credits

Sachin Kumar â€“ Project Developer

IIIT Guwahati â€“ Academic Institution

Libraries Used:

Coqui TTS

SpeechBrain

Streamlit

## ğŸ“· Screenshots



## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.


